{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d43b6f32",
   "metadata": {},
   "source": [
    "## Importaciones\n",
    "\n",
    "**re:** expresiones regulares.\n",
    "\n",
    "**unicodedata:** da acceso a la base de datos de caracteres Unicode. Su función principal es trabajar con las propiedades de los caracteres, especialmente para normalizarlos. Un uso muy común es para eliminar acentos y diacríticos \n",
    "\n",
    "**ntlk:** siglas de Natural Language Toolkit. Una de las bibliotecas más famosas y completas para el PLN. Sirve para análisis de texto y lingüística  computacional, como tokenización, POS tagging, lematización y derivación (stemming), análisis de sentimiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d016d3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "import nltk\n",
    "# import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5993fd",
   "metadata": {},
   "source": [
    "## Descargar \"paquetes de datos\"\n",
    "\n",
    "NLTK los necesita para poder realizar ciertas tareas.\n",
    "\n",
    "- **punkt:** contiene modelos pre-entrenados que le enseñan a NLTK a realizar la tokenización. Permite dividir un texto en una lista de oraciones, y luego esas oracoines en una lista de palabras, signos de puntuación, etc.\n",
    "\n",
    "- **punkt_tab:** es un archivo de datos interno del que depende punkt.\n",
    "\n",
    "- **stopwords:** descarga una lista de empty words para múltiples idiomas. Filtra y elimina estas palabras comunes para que el análisis se centre en palabras que realmente importan.\n",
    "\n",
    "- **wordnet:** es una enorme base de datos léxica del inglés similar a un diccionario o tesauro (obra de referencia que lista palabras agrupadas según la similitud de su significado) muy avanzado. Agrupa las palabras en conjuntos de sinónimos llamados synsets. Se usa para lematización (reducir una palabra a su forma base), encontrar sinónimos y antónimos, entender las relaciones semánticas entre palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918d1f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Aleex\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Aleex\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Aleex\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Aleex\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Descargar recursos la primera vez\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"punkt_tab\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "# Cargar el modelo de español de spaCy\n",
    "# nlp = spacy.load(\"es_core_news_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2e1b29",
   "metadata": {},
   "source": [
    "## Importación de herramientas\n",
    "\n",
    "Importan herramientas específicas de NLTK para preparar y limpiar texto antes de analizarlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e770af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords # Lista de empty words\n",
    "from nltk.tokenize import word_tokenize # Función de word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer # Clase WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16b6033d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "los autos son mas rapidos que las bicicletas pero las bicicletas son mas ecologicas\n"
     ]
    }
   ],
   "source": [
    "# Texto de ejemplo\n",
    "# \"¡Hola! Me gustan los programas de Inteligenciia Artificial, especialmente en 2025!\"\n",
    "# \"Yo no sé... ¿quizás los programas de I.A. (Inteligencia Artificial) corrían mejor en 2025?\"\n",
    "texto = \"Los autos son más RÁPIDOS que las bicicletas, pero las bicicletas son más ecológicas.\"\n",
    "\n",
    "# 1. Pasar a minúsculas\n",
    "texto = texto.lower()\n",
    "\n",
    "# 2. Eliminar puntuación\n",
    "texto = re.sub(r\"[^\\w\\s]\", \"\", texto) # re.sub(patron, reemplazo, cadena)\n",
    "# [^\\w\\s] el ^ indica que se busque todo lo que no esté dentro de los corchetes\n",
    "\n",
    "# 3. Eliminar acentos, diéresis, diacríticos\n",
    "texto = \"\".join(\n",
    "    c for c in unicodedata.normalize(\"NFD\", texto)\n",
    "    if unicodedata.category(c) != \"Mn\"\n",
    ") # separador.join(cadena) une los elementos de una cadena\n",
    "\n",
    "print(texto)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9bd07b",
   "metadata": {},
   "source": [
    "## Eliminar tildes, diéresis, diacríticos\n",
    "\n",
    "`unicodedata.normalize(\"NFD\", texto)`\n",
    "    Función que normaliza el texto a su Forma de Descomposición Normal, es decir, separa cada caracter acentuado en dos partes: la letra base y el diacrítico.\n",
    "\n",
    "`if unicodedata.category(c) != \"Mn\"`\n",
    "    Recorre cada uno de los caracteres del texto descompuesto. Revisa a qué categoría Unicode pertenece cada caracter, donde la categoría Mn corresponde a Mark, Nonspacing (marcas sin espaciado), que es precisamente la categoría de los acentos, tildes y diéresis. Entonces la condición significa: \"quédate con este carácter únicamente si NO es un acento/diacrítico\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59934a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['p', 'i', 'n', 'g', 'u', '̈', 'i', 'n', 'o']\n",
      "Ll\n",
      "Ll\n",
      "Ll\n",
      "Ll\n",
      "Ll\n",
      "Mn\n",
      "Ll\n",
      "Ll\n",
      "Ll\n",
      "['los', 'autos', 'son', 'mas', 'rapidos', 'que', 'las', 'bicicletas', 'pero', 'las', 'bicicletas', 'son', 'mas', 'ecologicas']\n"
     ]
    }
   ],
   "source": [
    "cadena = \"pingüino\"\n",
    "cadena = unicodedata.normalize(\"NFD\", cadena)\n",
    "print([i for i in cadena])\n",
    "\n",
    "for i in cadena:\n",
    "    print(unicodedata.category(i))\n",
    "    \n",
    "print(texto.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a660fc",
   "metadata": {},
   "source": [
    "## Tokenizar\n",
    "\n",
    "Toma la cadena de texto y la divide en una lista de sus componentes individuales.\n",
    "\n",
    "`language=\"spanish\"` le indica a la función que debe usar las reglas gramaticales y de puntuación del idioma español. Esto le ayuda a manejar correctamente cosas como los signos de apertura, que no existen en el inglés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f775420b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['los', 'autos', 'son', 'mas', 'rapidos', 'que', 'las', 'bicicletas', 'pero', 'las', 'bicicletas', 'son', 'mas', 'ecologicas']\n"
     ]
    }
   ],
   "source": [
    "# 4. Tokenizar\n",
    "tokens = word_tokenize(texto, language=\"spanish\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bb2240",
   "metadata": {},
   "source": [
    "## Empty words\n",
    "\n",
    "Da un conjunto de palabras vacías del español."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb3b9fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'habíais', 'estarán', 'fui', 'durante', 'estuvieseis', 'erais', 'sentidas', 'fueses', 'hemos', 'nosotras', 'seríamos', 'ellas', 'suyas', 'esos', 'estado', 'fuera', 'fuese', 'para', 'es', 'estuviéramos', 'me', 'teníamos', 'estad', 'serías', 'la', 'entre', 'hubimos', 'sois', 'sean', 'estabais', 'hubo', 'tuviese', 'tienes', 'estéis', 'e', 'algo', 'tuviesen', 'tuvierais', 'él', 'hube', 'estando', 'era', 'estará', 'fuésemos', 'habidos', 'estábamos', 'estaremos', 'tuvieras', 'tuvieron', 'ella', 'tuyas', 'había', 'quien', 'estemos', 'será', 'tengas', 'tuvieseis', 'una', 'nosotros', 'está', 'tuviera', 'estamos', 'estadas', 'tiene', 'eres', 'tuvieran', 'hubiesen', 'se', 'vosotros', 'sería', 'como', 'porque', 'teniendo', 'que', 'hay', 'tenida', 'tienen', 'muy', 'hubieseis', 'estaríamos', 'fuimos', 'fuisteis', 'ni', 'estuvo', 'habías', 'mías', 'tuyos', 'tuvimos', 'otras', 'suya', 'un', 'soy', 'has', 'tendrías', 'vuestro', 'sintiendo', 'eso', 'le', 'habríamos', 'por', 'qué', 'tenga', 'habrías', 'nos', 'estoy', 'fueras', 'fuiste', 'tendrá', 'tenían', 'otro', 'estuvieses', 'siente', 'tened', 'mis', 'fueron', 'tuya', 'tendremos', 'estada', 'no', 'tendrás', 'también', 'esas', 'estuviera', 'esto', 'fueran', 'hubiéramos', 'tendrían', 'tú', 'están', 'sin', 'con', 'tendré', 'hubiste', 'sentida', 'las', 'ante', 'han', 'lo', 'os', 'tenía', 'nada', 'tuvisteis', 'estuviesen', 'estar', 'estés', 'este', 'hubieras', 'tenido', 'nuestro', 'estuve', 'al', 'muchos', 'seremos', 'esté', 'tuviéramos', 'estarías', 'habréis', 'seamos', 'seréis', 'contra', 'estuvimos', 'fuéramos', 'quienes', 'pero', 'tuviésemos', 'tendría', 'fueseis', 'otros', 'ya', 'ellos', 'estabas', 'estas', 'tu', 'estuvierais', 'serían', 'hasta', 'tenéis', 'sus', 'hubieran', 'tengáis', 'estuvieran', 'habremos', 'sí', 'más', 'serán', 'he', 'hubisteis', 'habida', 'yo', 'fuerais', 'hubiera', 'esta', 'tendréis', 'y', 'estáis', 'ese', 'vuestra', 'seríais', 'estados', 'sobre', 'tus', 'cuando', 'tengan', 'tenidas', 'estuvisteis', 'tuviste', 'tanto', 'hayamos', 'tengo', 'eran', 'fue', 'antes', 'vosotras', 'estos', 'hubieron', 'habrá', 'tendríais', 'otra', 'nuestras', 'estarían', 'cual', 'seáis', 'eras', 'tendrán', 'estén', 'ti', 'desde', 'el', 'tuyo', 'habéis', 'nuestra', 'habidas', 'mucho', 'tuve', 'estuvieron', 'estaba', 'algunos', 'estuvieras', 'somos', 'son', 'hubierais', 'seré', 'tenías', 'hayáis', 'éramos', 'habrían', 'habrán', 'ha', 'habían', 'estaréis', 'suyo', 'habíamos', 'habiendo', 'estuviste', 'tuvo', 'poco', 'mi', 'unos', 'hubiésemos', 'todo', 'mí', 'vuestros', 'te', 'tuvieses', 'habido', 'hayan', 'estaré', 'estaría', 'habríais', 'suyos', 'de', 'todos', 'habrás', 'sentido', 'teníais', 'esa', 'serás', 'del', 'estaríais', 'hayas', 'habré', 'sea', 'tenidos', 'en', 'vuestras', 'estuviésemos', 'habría', 'nuestros', 'o', 'sentid', 'los', 'mío', 'uno', 'fuesen', 'tenemos', 'estaban', 'míos', 'seas', 'les', 'haya', 'hubieses', 'algunas', 'tengamos', 'hubiese', 'sentidos', 'donde', 'estás', 'a', 'mía', 'su', 'estuviese', 'estarás', 'tendríamos'}\n",
      "['autos', 'mas', 'rapidos', 'bicicletas', 'bicicletas', 'mas', 'ecologicas']\n"
     ]
    }
   ],
   "source": [
    "# 5. Eliminar stopwords (en español)\n",
    "stop_words = set(stopwords.words(\"spanish\"))\n",
    "print(stop_words)\n",
    "tokens = [t for t in tokens if t not in stop_words]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca70b359",
   "metadata": {},
   "source": [
    "## Lematización\n",
    "\n",
    "Convierte cada palabra de la lista tokens a su forma base. \n",
    "\n",
    "`lemmatizer = WordNetLemmatizer()` crea una instancia del lematizador de WordNet.\n",
    "\n",
    "`lemmatizer.lemmatize(t)` aplica la función de lematización a cada palabra (t). El lematizador busca la palabra en su diccionario (WordNet) y la devuelve en su forma raíz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d254b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto normalizado: ['auto', 'ma', 'rapidos', 'bicicletas', 'bicicletas', 'ma', 'ecologicas']\n"
     ]
    }
   ],
   "source": [
    "# 6. Lematización\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "\n",
    "print(\"Texto normalizado:\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0617205c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- MÉTODO 1: TOKENIZAR PRIMERO ---\n",
      "Paso 1 - Tokens iniciales: ['Yo', 'no', 'sé', '...', '¿quizás', 'la', 'I.A', '.', '(', 'Inteligencia', 'Artificial', ')', 'es', 'el', 'futuro-2.0', '?']\n",
      "\n",
      "Paso 2 - Tokens finales limpios: ['yo', 'no', 'se', '...', 'la', 'i.a', 'inteligencia', 'artificial', 'es', 'el', 'futuro-2.0']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# La cadena de prueba\n",
    "complicado1 = \"Yo no sé... ¿quizás la I.A. (Inteligencia Artificial) es el futuro-2.0?\"\n",
    "\n",
    "print(\"--- MÉTODO 1: TOKENIZAR PRIMERO ---\")\n",
    "\n",
    "# 1. Tokenizar el texto original\n",
    "tokens = nltk.word_tokenize(complicado1, language=\"spanish\")\n",
    "print(f\"Paso 1 - Tokens iniciales: {tokens}\\n\")\n",
    "\n",
    "# 2. Limpiar la lista de tokens\n",
    "tokens_limpios = []\n",
    "for token in tokens:\n",
    "    # Pasar a minúsculas\n",
    "    token_limpio = token.lower()\n",
    "    \n",
    "    # Quitar acentos\n",
    "    token_limpio = \"\".join(\n",
    "        c for c in unicodedata.normalize(\"NFD\", token_limpio)\n",
    "        if unicodedata.category(c) != \"Mn\"\n",
    "    )\n",
    "    \n",
    "    # Nos quedamos con tokens que no son solo puntuación y tienen contenido\n",
    "    # (Esto conserva \"i.a.\" y \"futuro-2.0\" pero elimina \"...\" o \"?\")\n",
    "    if token_limpio.isalnum() or '.' in token_limpio or '-' in token_limpio:\n",
    "        if len(token_limpio) > 1 or token_limpio.isalnum():\n",
    "             tokens_limpios.append(token_limpio)\n",
    "\n",
    "print(f\"Paso 2 - Tokens finales limpios: {tokens_limpios}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ddf6d334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- MÉTODO 2: TOKENIZAR AL FINAL ---\n",
      "Paso 1 - Texto completamente limpio: 'yo no se quizas la ia inteligencia artificial es el futuro20'\n",
      "\n",
      "Paso 2 - Tokens resultantes: ['yo', 'no', 'se', 'quizas', 'la', 'ia', 'inteligencia', 'artificial', 'es', 'el', 'futuro20']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"--- MÉTODO 2: TOKENIZAR AL FINAL ---\")\n",
    "\n",
    "# 1. Pasar a minúsculas\n",
    "texto_limpio = complicado1.lower()\n",
    "\n",
    "# 2. Eliminar puntuación\n",
    "texto_limpio = re.sub(r\"[^\\w\\s]\", \"\", texto_limpio)\n",
    "\n",
    "# 3. Eliminar acentos\n",
    "texto_limpio = \"\".join(\n",
    "    c for c in unicodedata.normalize(\"NFD\", texto_limpio)\n",
    "    if unicodedata.category(c) != \"Mn\"\n",
    ")\n",
    "print(f\"Paso 1 - Texto completamente limpio: '{texto_limpio}'\\n\")\n",
    "\n",
    "\n",
    "# 4. Tokenizar el texto ya limpio\n",
    "tokens_finales = nltk.word_tokenize(texto_limpio, language=\"spanish\")\n",
    "print(f\"Paso 2 - Tokens resultantes: {tokens_finales}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24929eac",
   "metadata": {},
   "source": [
    "## El Orden Recomendado (Más Robusto)\n",
    "\n",
    "La lógica es:\n",
    "\n",
    "Tokenizar: Divide el texto crudo en sus componentes básicos.\n",
    "\n",
    "Limpiar y Filtrar: Recorre la lista de tokens y elimina los que no sirven (puntuación, stopwords).\n",
    "\n",
    "Normalizar: Aplica la lematización a los tokens de palabras que quedaron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1342f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto1 original: Yo no sé... ¿quizás los programas de I.A. (Inteligencia Artificial) corrían mejor en 2025?\n",
      "\n",
      "Tokens iniciales: ['Yo', 'no', 'sé', '...', '¿quizás', 'los', 'programas', 'de', 'I.A', '.', '(', 'Inteligencia', 'Artificial', ')', 'corrían', 'mejor', 'en', '2025', '?']\n",
      "\n",
      "Texto final normalizado: ['programas', 'inteligencia', 'artificial', 'corrian', 'mejor']\n"
     ]
    }
   ],
   "source": [
    "texto1 = \"Yo no sé... ¿quizás los programas de I.A. (Inteligencia Artificial) corrían mejor en 2025?\"\n",
    "\n",
    "# Paso 1: Tokenizar el texto1 original\n",
    "# Esto preserva la estructura antes de que la limpieza la destruya.\n",
    "tokens = word_tokenize(texto1, language=\"spanish\")\n",
    "\n",
    "# Paso 2: Pre-cargar las stopwords y el lematizador\n",
    "# Es más eficiente hacerlo una sola vez fuera del bucle.\n",
    "stop_words = set(stopwords.words(\"spanish\"))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Paso 3: Limpiar y normalizar la lista de tokens\n",
    "tokens_limpios_y_lematizados = []\n",
    "for token in tokens:\n",
    "    # 3a: Pasar a minúsculas\n",
    "    token = token.lower()\n",
    "\n",
    "    # 3b: Eliminar acentos\n",
    "    token = \"\".join(\n",
    "        c for c in unicodedata.normalize(\"NFD\", token)\n",
    "        if unicodedata.category(c) != \"Mn\"\n",
    "    )\n",
    "\n",
    "    # 3c: Filtrar tokens que son puntuación O stopwords\n",
    "    # Nos quedamos solo con tokens alfabéticos que no sean stopwords.\n",
    "    if token.isalpha() and token not in stop_words:\n",
    "        # 3d: Lematizar el token que pasó todos los filtros\n",
    "        token_lematizado = lemmatizer.lemmatize(token)\n",
    "        tokens_limpios_y_lematizados.append(token_lematizado)\n",
    "\n",
    "\n",
    "print(\"Texto1 original:\", texto1)\n",
    "print(\"\\nTokens iniciales:\", tokens)\n",
    "print(\"\\nTexto final normalizado:\", tokens_limpios_y_lematizados)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
